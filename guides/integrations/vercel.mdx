---
title: "Vercel Deployment Guide"
description: "Complete guide to deploying SundayPyjamas AI Suite applications on Vercel"
---

## Overview

This guide shows you how to deploy your SundayPyjamas AI Suite applications on Vercel, leveraging Vercel's serverless functions, edge network, and AI-first infrastructure for optimal performance.

## Why Vercel for AI Applications?

Vercel offers several advantages for AI-powered applications:

- **Serverless Functions**: Scale automatically based on demand
- **Edge Network**: 35+ global regions for low-latency responses
- **AI-First Infrastructure**: Built-in support for AI workloads
- **Automatic Scaling**: Handle traffic spikes without configuration
- **Zero Cold Starts**: Functions stay warm for better performance
- **Built-in Caching**: Optimize API responses automatically

## Prerequisites

- Vercel account (free tier available)
- SundayPyjamas AI Suite account and API key
- Git repository with your application code
- Vercel CLI (optional, for local development)

## Quick Deploy Options

### 1. Vercel Marketplace (Recommended)

The fastest way to deploy is using our official Vercel template:

1. **Visit the Template**: Go to [SundayPyjamas AI Suite Template](https://vercel.com/templates/ai/sundaypyjamas-ai-suite)
2. **Click "Deploy"**: One-click deployment to your Vercel account
3. **Configure Environment Variables**: Add your API key
4. **Deploy**: Your application will be live in minutes

### 2. Manual Deployment

#### Step 1: Connect Your Repository

```bash
# Install Vercel CLI
npm i -g vercel

# Login to Vercel
vercel login

# Link your project
vercel link
```

#### Step 2: Configure Environment Variables

```bash
# Set environment variables
vercel env add SUNDAYPYJAMAS_API_KEY
# Enter: spj_ai_your_api_key_here

# For client-side usage
vercel env add NEXT_PUBLIC_SUNDAYPYJAMAS_API_KEY
# Enter: spj_ai_your_api_key_here
```

#### Step 3: Deploy

```bash
# Deploy to production
vercel --prod
```

## Application Examples

### 1. Next.js Chat Application

#### Project Structure

```
nextjs-chat/
├── app/
│   ├── api/
│   │   └── chat/
│   │       └── route.js
│   ├── components/
│   │   ├── ChatInterface.jsx
│   │   └── Message.jsx
│   ├── page.js
│   └── layout.js
├── lib/
│   └── sundaypyjamas.js
├── package.json
└── vercel.json
```

#### API Route (`app/api/chat/route.js`)

```javascript
import { NextResponse } from 'next/server';

export async function POST(request) {
  try {
    const { messages } = await request.json();

    const response = await fetch('https://suite.sundaypyjamas.com/api/v1/chat', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.SUNDAYPYJAMAS_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages,
        model: 'llama-3.3-70b-versatile',
        temperature: 0.7,
      })
    });

    const data = await response.json();
    return NextResponse.json(data);

  } catch (error) {
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    );
  }
}

export const config = {
  runtime: 'edge', // Use Edge Runtime for better performance
};
```

#### Vercel Configuration (`vercel.json`)

```json
{
  "functions": {
    "app/api/chat/route.js": {
      "runtime": "@vercel/node"
    }
  },
  "regions": ["iad1", "sfo1", "fra1"], // Specify regions for your users
  "env": {
    "SUNDAYPYJAMAS_API_KEY": "@sundaypyjamas-api-key"
  },
  "buildCommand": "npm run build",
  "devCommand": "npm run dev",
  "installCommand": "npm install"
}
```

### 2. FastAPI Application

#### Project Structure

```
fastapi-ai/
├── app/
│   ├── main.py
│   ├── config.py
│   └── routes/
│       └── chat.py
├── requirements.txt
├── vercel.json
└── api/
    └── index.py
```

#### Vercel API Handler (`api/index.py`)

```python
from fastapi import FastAPI
from app.main import app as fastapi_app
import os

# Vercel expects an ASGI app
app = fastapi_app

# For Vercel serverless functions
def handler(event, context):
    return app(event, context)
```

#### Requirements (`requirements.txt`)

```txt
fastapi==0.104.1
uvicorn==0.24.0
sundaypyjamas-ai==1.0.0
pydantic==2.5.0
pydantic-settings==2.1.0
```

#### Vercel Configuration (`vercel.json`)

```json
{
  "functions": {
    "api/index.py": {
      "runtime": "@vercel/python"
    }
  },
  "builds": [
    {
      "src": "api/index.py",
      "use": "@vercel/python"
    }
  ],
  "routes": [
    {
      "src": "/api/(.*)",
      "dest": "/api/index.py"
    }
  ],
  "env": {
    "SUNDAYPYJAMAS_API_KEY": "@sundaypyjamas-api-key"
  }
}
```

### 3. Edge Function for Streaming

#### Edge API Route (`app/api/chat/stream/route.js`)

```javascript
export default async function handler(request) {
  const { messages } = await request.json();

  const response = await fetch('https://suite.sundaypyjamas.com/api/v1/chat', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUNDAYPYJAMAS_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      messages,
      stream: true,
      model: 'llama-3.3-70b-versatile',
    })
  });

  return new Response(response.body, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}

export const config = {
  runtime: 'edge',
};
```

## Advanced Vercel Features

### 1. Edge Runtime Optimization

#### Edge-Optimized AI Client

```javascript
// lib/edge-client.js
export class EdgeAIClient {
  constructor(apiKey) {
    this.apiKey = apiKey;
    this.baseURL = 'https://suite.sundaypyjamas.com/api/v1';
  }

  async chat(messages, options = {}) {
    const response = await fetch(`${this.baseURL}/chat`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages,
        model: options.model || 'llama-3.3-70b-versatile',
        temperature: options.temperature || 0.7,
        max_tokens: options.maxTokens || 1000,
      })
    });

    return await response.json();
  }

  async *chatStream(messages, options = {}) {
    const response = await fetch(`${this.baseURL}/chat`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages,
        stream: true,
        ...options
      })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          if (data === '[DONE]') return;

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices[0]?.delta?.content;
            if (content) {
              yield content;
            }
          } catch (e) {
            // Skip invalid JSON
          }
        }
      }
    }
  }
}
```

### 2. ISR (Incremental Static Regeneration)

```javascript
// app/blog/[slug]/page.js
export async function generateStaticParams() {
  // Generate static pages for blog posts
  return [
    { slug: 'getting-started' },
    { slug: 'advanced-features' },
  ];
}

export default function BlogPost({ params }) {
  return (
    <div>
      <h1>Blog Post: {params.slug}</h1>
      {/* Content will be generated dynamically */}
    </div>
  );
}

// Revalidate every 60 seconds
export const revalidate = 60;
```

### 3. Image Optimization with AI

```javascript
// app/api/generate-image/route.js
import { NextResponse } from 'next/server';

export async function POST(request) {
  const { prompt } = await request.json();

  // Generate image description using AI
  const aiResponse = await fetch('https://suite.sundaypyjamas.com/api/v1/chat', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.SUNDAYPYJAMAS_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      messages: [
        {
          role: 'user',
          content: `Create a detailed description for an image based on: ${prompt}`
        }
      ]
    })
  });

  const aiData = await aiResponse.json();
  const imageDescription = aiData.choices[0].message.content;

  // Use the description with an image generation service
  // (e.g., DALL-E, Stable Diffusion, etc.)

  return NextResponse.json({
    description: imageDescription,
    // imageUrl: generatedImageUrl
  });
}
```

## Performance Optimization

### 1. Function Optimization

#### Warm Functions

```javascript
// vercel.json
{
  "functions": {
    "app/api/chat/route.js": {
      "runtime": "@vercel/node",
      "maxDuration": 30,
      "memory": 1024
    }
  }
}
```

#### Connection Reuse

```javascript
// lib/connection-pool.js
class ConnectionPool {
  constructor() {
    this.connections = new Map();
  }

  async getConnection(endpoint) {
    if (!this.connections.has(endpoint)) {
      // Create persistent connection
      this.connections.set(endpoint, createConnection(endpoint));
    }
    return this.connections.get(endpoint);
  }
}
```

### 2. Caching Strategies

#### API Response Caching

```javascript
// lib/cache.js
import { kv } from '@vercel/kv';

export class VercelKVCache {
  static async get(key) {
    return await kv.get(key);
  }

  static async set(key, value, ttl = 300) {
    return await kv.set(key, value, { ex: ttl });
  }

  static generateKey(messages, options = {}) {
    const content = JSON.stringify({ messages, options });
    return `ai_cache:${Buffer.from(content).toString('base64')}`;
  }
}
```

#### ISR with AI Content

```javascript
// app/blog/page.js
export default async function BlogPage() {
  // Generate AI-powered content at build time
  const aiContent = await generateAIContent();

  return (
    <div>
      <h1>AI-Generated Blog</h1>
      <div dangerouslySetInnerHTML={{ __html: aiContent }} />
    </div>
  );
}

// Revalidate every hour
export const revalidate = 3600;
```

### 3. Monitoring & Analytics

#### Vercel Analytics

```javascript
// app/layout.js
import { Analytics } from '@vercel/analytics/react';

export default function RootLayout({ children }) {
  return (
    <html lang="en">
      <body>
        {children}
        <Analytics />
      </body>
    </html>
  );
}
```

#### Custom Metrics

```javascript
// lib/metrics.js
export function trackAIMetrics(endpoint, duration, success, tokens = null) {
  // Track AI-specific metrics
  if (typeof window !== 'undefined' && window.gtag) {
    window.gtag('event', 'ai_api_call', {
      event_category: 'ai',
      event_label: endpoint,
      value: duration,
      custom_parameter_1: success ? 'success' : 'error',
      custom_parameter_2: tokens
    });
  }
}
```

## Security Best Practices

### 1. Environment Variables

```bash
# Never commit API keys to version control
vercel env add SUNDAYPYJAMAS_API_KEY
vercel env add NEXT_PUBLIC_SUNDAYPYJAMAS_API_KEY

# Use different keys for different environments
vercel env add SUNDAYPYJAMAS_API_KEY production
```

### 2. API Route Protection

```javascript
// app/api/chat/route.js
export async function POST(request) {
  // Validate request origin
  const origin = request.headers.get('origin');
  const allowedOrigins = [
    'https://yourdomain.vercel.app',
    'http://localhost:3000'
  ];

  if (!allowedOrigins.includes(origin)) {
    return NextResponse.json(
      { error: 'Unauthorized origin' },
      { status: 403 }
    );
  }

  // Continue with normal processing
}
```

### 3. Rate Limiting

```javascript
// lib/rate-limiter.js
import { kv } from '@vercel/kv';

export class RateLimiter {
  static async checkLimit(identifier, limit = 100, window = 60) {
    const key = `rate_limit:${identifier}`;
    const current = await kv.get(key) || 0;

    if (current >= limit) {
      return false;
    }

    await kv.set(key, current + 1, { ex: window });
    return true;
  }
}
```

## Deployment Strategies

### 1. Preview Deployments

```bash
# Deploy to preview environment
vercel

# Promote to production
vercel --prod
```

### 2. Branch-based Deployments

```json
// vercel.json
{
  "git": {
    "deploymentEnabled": {
      "main": true,
      "develop": true,
      "feature/*": false
    }
  }
}
```

### 3. Environment-specific Configuration

```javascript
// lib/config.js
export const config = {
  development: {
    apiUrl: 'http://localhost:3000',
    aiModel: 'llama-3.3-70b-versatile'
  },
  production: {
    apiUrl: process.env.VERCEL_URL,
    aiModel: 'llama-3.3-70b-versatile'
  }
};

export const currentConfig = config[process.env.NODE_ENV] || config.production;
```

## Troubleshooting

### Common Issues

<AccordionGroup>
<Accordion title="Deployment Failures">
- Check build logs in Vercel dashboard
- Verify all dependencies are listed in package.json
- Ensure environment variables are set correctly
- Check for syntax errors in your code
</Accordion>

<Accordion title="Function Timeout">
- Increase function timeout in vercel.json
- Optimize API calls and reduce processing time
- Use streaming for long-running operations
- Implement proper error handling
</Accordion>

<Accordion title="Cold Start Issues">
- Use Edge Runtime for better performance
- Implement connection pooling
- Cache frequently used data
- Optimize bundle size
</Accordion>

<Accordion title="Environment Variable Issues">
- Verify environment variables are set in Vercel dashboard
- Check variable names match your code
- Ensure variables are available in the correct environment
- Use vercel env pull to sync local environment
</Accordion>
</AccordionGroup>

## Monitoring & Debugging

### 1. Vercel Dashboard

- **Functions Tab**: Monitor function execution and errors
- **Deployments Tab**: View build logs and deployment history
- **Analytics Tab**: Monitor performance and usage metrics
- **Settings Tab**: Configure environment variables and domains

### 2. Logging

```javascript
// lib/logger.js
export class VercelLogger {
  static log(level, message, context = {}) {
    const logData = {
      level,
      message,
      timestamp: new Date().toISOString(),
      context
    };

    // Vercel will capture console logs
    console[level](JSON.stringify(logData));
  }

  static info(message, context) {
    this.log('info', message, context);
  }

  static error(message, context) {
    this.log('error', message, context);
  }

  static warn(message, context) {
    this.log('warn', message, context);
  }
}
```

## Best Practices

### Performance
- Use Edge Runtime for global applications
- Implement proper caching strategies
- Optimize bundle size and loading times
- Use streaming for better user experience
- Monitor and optimize API usage

### Security
- Never expose API keys in client-side code
- Use environment variables for sensitive data
- Implement proper authentication and authorization
- Keep dependencies updated
- Monitor for security vulnerabilities

### Development
- Use preview deployments for testing
- Implement proper error handling
- Use TypeScript for better code quality
- Follow Vercel's best practices
- Use Vercel CLI for local development

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Next.js Integration"
    icon="code"
    href="/guides/integrations/nextjs"
  >
    Learn advanced Next.js integration patterns.
  </Card>
  <Card
    title="Monitoring Setup"
    icon="gauge-high"
    href="/platform/monitoring/datadog"
  >
    Set up comprehensive monitoring for your Vercel applications.
  </Card>
  <Card
    title="Security Best Practices"
    icon="shield-check"
    href="/platform/security/authentication"
  >
    Implement security best practices for your applications.
  </Card>
  <Card
    title="Advanced Features"
    icon="bolt"
    href="/guides/advanced/streaming-patterns"
  >
    Explore advanced Vercel features for AI applications.
  </Card>
</CardGroup>

<Note>
Vercel provides an excellent platform for deploying AI-powered applications with SundayPyjamas AI Suite. Its serverless architecture, edge network, and built-in optimizations make it ideal for scalable AI applications with low latency and high performance.
</Note>
